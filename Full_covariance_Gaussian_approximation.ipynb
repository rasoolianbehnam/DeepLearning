{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "feedforward",
      "language": "python",
      "name": "feedforward"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Full covariance Gaussian approximation.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rasoolianbehnam/DeepLearning/blob/master/Full_covariance_Gaussian_approximation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bne_Hu2L8opY"
      },
      "source": [
        "# Full covariance Gaussian approximation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEISgi1F8opf"
      },
      "source": [
        "This reading follows on from the previous coding tutorial, and shows how to set up a full covariance variational approximating Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K4Jn6EQ8opg"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"TFP version:\", tfp.__version__)\n",
        "\n",
        "# Additional packages for this reading\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPmIVdc18opj"
      },
      "source": [
        "In the previous tutorial, you approximated a full covariance Gaussian with a diagonal covariance Gaussian. This reading notebook is a supplement that shows how you can configure a full covariance Gaussian as the approximating distribution.\n",
        "\n",
        "The code below initializes the same target distribution that you saw in the tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krmj3a3W8opj"
      },
      "source": [
        "# Define the target distribution\n",
        "\n",
        "tf.random.set_seed(41)\n",
        "\n",
        "p_mu = [0., 0.]\n",
        "p_Sigma = tfp.bijectors.Chain([tfb.TransformDiagonal(tfb.Softplus()),\n",
        "                               tfb.FillTriangular()])(tf.random.uniform([3]))\n",
        "p = tfd.MultivariateNormalTriL(loc=p_mu, scale_tril=p_Sigma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wgzm75LK8opk"
      },
      "source": [
        "To create a trainable normal distribution with full covariance matrix, we use the `MultivariateNormalTriL` Distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zb_JvLzh8opl"
      },
      "source": [
        "# Define the approximating distribution\n",
        "\n",
        "scale_tril_init = tfb.FillScaleTriL()(tf.random.normal([3]))\n",
        "q = tfd.MultivariateNormalTriL(loc=tf.Variable(tf.random.normal([2])),\n",
        "                               scale_tril=tfp.util.TransformedVariable(scale_tril_init,\n",
        "                                                                       bijector=tfb.FillScaleTriL()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCspZZrO8opm"
      },
      "source": [
        "As with `MultivariateNormalDiag`, `loc` is specified as a randomly intialized `tf.Variable`.\n",
        "\n",
        "The lower-triangular matrix `scale_tril`, on the other hand, is initialized as\n",
        "```\n",
        "    tfp.util.TransformedVariable(scale_tril_init,\n",
        "                                 bijector=tfb.FillScaleTriL())\n",
        "```\n",
        "Let's unpack this bit by bit. `tfp.util.TransformedVariable` is a class that allows us to initialize a Variable using a value for its bijection. Parameter updates take place on the unconstrained Variable, while the bijection enforces a constraint (e.g. positivity, shape, etc.).\n",
        "\n",
        "In this case, we initialize `scale_tril` using a lower-triangular matrix, `scale_tril_init`.\n",
        "\n",
        "The bijector handed to `TransformedVariable` is `tfb.FillScaleTriL`. This bijector is equivalent to a `tfb.Chain` of `tfb.FillTriangular` followed by `tfb.TransformDiagonal`.\n",
        "\n",
        "`tfb.FillTriangular` inserts the elements of a vector into a lower-triangular matrix in a clockwise spiral. `tfb.TransformDiagonal` then applies a bijection to the diagonal of this matrix. \n",
        "\n",
        "The diagonal bijection applied by `tfb.FillScaleTriL` can be specified via the `diag_bijector` argument. By default, it is a bijector chain of `tfb.Softplus` followed by addition of `1e-5`.\n",
        "\n",
        "If you refer back to how `p_Sigma` is declared in the code cell above, you can see that it is initialized using a bijector chain that is similar to `tfb.FillScaleTriL`'s."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bm-gZF0e8opp"
      },
      "source": [
        "#### Fit the approximating distribution by minimising KL divergence\n",
        "\n",
        "The target and trainable distributions have been initialized. All that remains is to fit the trainable distribution to the target. \n",
        "\n",
        "The code below is identical to what you saw for fitting the diagonal covariance example in the coding tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adQ-xnH88opq"
      },
      "source": [
        "@tf.function\n",
        "def loss_and_grads(dist_a, dist_b):\n",
        "    '''\n",
        "        Returns D_{KL}[dist_a || dist_b] and the gradients of this \n",
        "        with respect to the trainable Variables of dist_a.\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = tfd.kl_divergence(dist_a, dist_b)\n",
        "    return loss, tape.gradient(loss, dist_a.trainable_variables)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_5u5gvE8opr"
      },
      "source": [
        "# Define function for graphics\n",
        "\n",
        "def plot_density_contours(density, X1, X2, contour_kwargs, ax=None):\n",
        "    '''\n",
        "        Plots the contours of a bivariate TensorFlow density function (i.e. .prob()).\n",
        "        X1 and X2 are numpy arrays of mesh coordinates.\n",
        "    '''\n",
        "    X = np.hstack([X1.flatten()[:, np.newaxis], X2.flatten()[:, np.newaxis]])\n",
        "    density_values = np.reshape(density(X).numpy(), newshape=X1.shape)\n",
        "    \n",
        "    if ax==None:\n",
        "        _, ax = plt.subplots(figsize=(7, 7))\n",
        "    \n",
        "    ax.contour(X1, X2, density_values, **contour_kwargs)\n",
        "    return(ax)\n",
        "\n",
        "x1 = np.linspace(-5, 5, 1000)\n",
        "x2 = np.linspace(-5, 5, 1000)\n",
        "X1, X2 = np.meshgrid(x1, x2)\n",
        "contour_levels = np.linspace(1e-4, 10**(-0.8), 20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OX7V11FJ8ops"
      },
      "source": [
        "# Set up and run a custom training loop to minimise the KL loss\n",
        "\n",
        "num_train_steps = 1000\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=.01)\n",
        "for i in range(num_train_steps):\n",
        "    \n",
        "    # Compute the KL divergence and its gradients\n",
        "    q_loss, grads = loss_and_grads(q, p)\n",
        "    \n",
        "    # Update the trainable variables using the gradients via the optimizer\n",
        "    opt.apply_gradients(zip(grads, q.trainable_variables))\n",
        "    \n",
        "    # Plot the updated density \n",
        "    if ((i + 1) % 10 == 0):\n",
        "        clear_output(wait=True)\n",
        "        ax = plot_density_contours(p.prob, X1, X2,\n",
        "                                   {'levels':contour_levels,\n",
        "                                    'cmap':'cividis', 'alpha':0.5})\n",
        "        ax = plot_density_contours(q.prob, X1, X2, \n",
        "                                   {'levels':contour_levels,\n",
        "                                    'cmap':'plasma'}, ax=ax)\n",
        "        ax.set_title('Density contours of $p$ and $q$\\n' +\n",
        "                     'Iteration ' + str(i + 1) + '\\n' +\n",
        "                      '$D_{KL}[q \\ || \\ p] = ' + \n",
        "                      str(np.round(q_loss.numpy(), 4)) + '$',\n",
        "                      loc='left')\n",
        "        plt.pause(.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAl0MfcO8ops"
      },
      "source": [
        "As you can see, using a trainable distribution that is in the same parametric family as the target enables the KL divergence to be minimised to zero, indicating that the target distribution has been learnt perfectly.\n",
        "\n",
        "### Summary\n",
        "\n",
        "This notebook showed how you can initialize and train a normal distribution with full covariance matrix. `MultivariateNormalTriL` with a Variable transformed via `FillScaleTriL` should be your go-to for learnt full covariance matrices."
      ]
    }
  ]
}